\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    showstringspaces=false
}

\title{CS 771 Assignment 3: FCOS Object Detector\\Part 1 - Model Inference}
\author{Team Member Names}
\date{\today}

\begin{document}

\maketitle

\section{Overview}

This report describes our implementation of the inference procedure for FCOS (Fully Convolutional One-Stage Object Detection), including the classification and regression heads, object decoding, and evaluation results on the PASCAL VOC 2007 test set.

\section{Implementation Details}

\subsection{Classification and Regression Heads (2 pts)}

\subsubsection{Classification Head}

The classification head (\texttt{FCOSClassificationHead}) is implemented in \texttt{code/libs/model.py} (lines 23-83). The design follows the FCOS architecture with:

\begin{itemize}
    \item \textbf{Shared Convolutional Layers}: 3 convolutional layers (3$\times$3 kernels) with GroupNorm (16 groups) and ReLU activation
    \item \textbf{Final Classification Layer}: A 3$\times$3 convolution producing $C$ logits per location (where $C=20$ for VOC)
    \item \textbf{Initialization}: The final layer bias is initialized to $-\log\left(\frac{1-p}{p}\right)$ where $p=0.01$, following the Focal Loss paper to prevent the large number of background locations from overwhelming the loss during training
\end{itemize}

The forward pass applies the shared convolutions followed by the classification layer to each feature pyramid level, producing a list of tensors with shape $N \times C \times H \times W$.

\subsubsection{Regression Head}

The regression head (\texttt{FCOSRegressionHead}) is implemented in \texttt{code/libs/model.py} (lines 86-161). It predicts both bounding box offsets and centerness scores:

\begin{itemize}
    \item \textbf{Shared Convolutional Layers}: Similar to the classification head, 3 convolutional layers with GroupNorm and ReLU
    \item \textbf{Bounding Box Regression}: A 3$\times$3 convolution producing 4 values per location (left, top, right, bottom distances), followed by ReLU to ensure positive outputs
    \item \textbf{Centerness Prediction}: A separate 3$\times$3 convolution producing 1 value per location to predict center-ness
\end{itemize}

The forward pass returns two lists: regression outputs ($N \times 4 \times H \times W$) and centerness logits ($N \times 1 \times H \times W$) for each pyramid level.

\subsection{Inference Procedure (3 pts)}

The inference procedure is implemented in the \texttt{inference} method of the FCOS class (\texttt{code/libs/model.py}, lines 441-602). The implementation follows these steps for each image:

\subsubsection{Score Computation}

For each pyramid level, we compute detection scores by combining classification and centerness predictions:

\begin{equation}
    \text{score} = \sqrt{\sigma(\text{cls\_logit}) \times \sigma(\text{ctr\_logit})}
\end{equation}

where $\sigma$ is the sigmoid function. The square root is used to balance the contribution of classification confidence and localization quality, following both the official torchvision implementation and the original author's implementation.

\subsubsection{Candidate Filtering and Selection}

\begin{enumerate}
    \item \textbf{Score Thresholding}: Filter locations where the maximum class score exceeds \texttt{score\_thresh} (0.1). This is present in both official implementations as \texttt{pre\_nms\_thresh} (author's code) and \texttt{score\_thresh} (torchvision).

    \textit{Source}: \texttt{inference.py} line 73 in \url{https://github.com/tianzhi0549/FCOS}

    \item \textbf{Top-K Selection}: Select up to \texttt{topk\_candidates} (1000) highest-scoring predictions across all classes before NMS. This reduces computational cost during NMS by limiting the number of candidate boxes.

    \textit{Source}: \texttt{inference.py} lines 88-92 in author's implementation, \texttt{postprocess\_detections} in torchvision
\end{enumerate}

\subsubsection{Box Decoding}

Bounding boxes are decoded from center points and predicted offsets using:

\begin{align}
    x_1 &= x_{\text{center}} - \text{left} \times \text{stride} \\
    y_1 &= y_{\text{center}} - \text{top} \times \text{stride} \\
    x_2 &= x_{\text{center}} + \text{right} \times \text{stride} \\
    y_2 &= y_{\text{center}} + \text{bottom} \times \text{stride}
\end{align}

where the regression outputs are multiplied by the feature stride since they were divided by the stride during training (for scale normalization).

\subsubsection{Post-Processing}

\begin{enumerate}
    \item \textbf{Box Clipping}: Clip boxes to image boundaries to handle predictions that extend beyond the image due to padding or stride effects. Present in both implementations.

    \textit{Source}: \texttt{boxlist.clip\_to\_image()} in author's code, \texttt{box\_ops.clip\_boxes\_to\_image()} in torchvision

    \item \textbf{Small Box Removal}: Remove boxes with width or height $\leq$ 1.0 pixel to filter out degenerate predictions. This is present in the author's implementation.

    \textit{Source}: \texttt{remove\_small\_boxes(boxlist, self.min\_size)} in author's \texttt{inference.py} line 103

    \item \textbf{Per-Class NMS}: Apply non-maximum suppression using \texttt{batched\_nms} with IoU threshold of 0.6. This performs NMS separately for each class to allow overlapping detections from different categories.

    \textit{Source}: Present in both implementations

    \item \textbf{Top-N Selection}: Keep top \texttt{detections\_per\_img} (100) detections after NMS

    \item \textbf{Label Offsetting}: Add 1 to predicted class indices to account for background removal during training (COCO format convention)
\end{enumerate}

\subsection{Implementation Verification}

All design decisions were verified against official implementations:

\begin{itemize}
    \item \textbf{PyTorch Torchvision}: \url{https://pytorch.org/vision/main/_modules/torchvision/models/detection/fcos.html}
    \item \textbf{Original Author's Code}: \url{https://github.com/tianzhi0549/FCOS}
\end{itemize}

Key implementation details match the official sources, including:
\begin{itemize}
    \item Score computation using $\sqrt{\text{cls} \times \text{ctr}}$
    \item Box decoding formula
    \item Per-class NMS using \texttt{batched\_nms}
    \item Coordinate order: points stored as (x, y) in \texttt{point\_generator.py}
\end{itemize}

\section{Evaluation Results}

\subsection{Test Configuration}

The pretrained ResNet-18 FCOS model was evaluated on the PASCAL VOC 2007 test set with the following parameters:

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Backbone & ResNet-18 \\
FPN Feature Dimension & 128 \\
Score Threshold & 0.1 \\
NMS Threshold & 0.6 \\
Top-K Candidates & 1000 \\
Detections Per Image & 100 \\
\bottomrule
\end{tabular}
\caption{Evaluation configuration}
\end{table}

\subsection{mAP Scores}

The evaluation results on the PASCAL VOC 2007 test set are:

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
AP @ IoU=0.50:0.95 & 0.326 (32.6\%) \\
\textbf{AP @ IoU=0.50} & \textbf{0.606 (60.6\%)} \\
AP @ IoU=0.75 & 0.315 (31.5\%) \\
AP (small objects) & 0.072 (7.2\%) \\
AP (medium objects) & 0.199 (19.9\%) \\
AP (large objects) & 0.404 (40.4\%) \\
\midrule
AR @ maxDets=1 & 0.316 (31.6\%) \\
AR @ maxDets=10 & 0.493 (49.3\%) \\
AR @ maxDets=100 & 0.529 (52.9\%) \\
\bottomrule
\end{tabular}
\caption{COCO-style evaluation metrics on PASCAL VOC 2007 test set}
\end{table}

The achieved \textbf{mAP@IoU=0.5 of 60.6\%} closely matches the expected performance of $\sim$61\% mentioned in the pretrained model configuration, confirming correct implementation.

\subsection{Inference Time}

Total inference time on the PASCAL VOC 2007 test set (4952 images): \textbf{94.39 seconds}

Average inference time per image: $\frac{94.39}{4952} \approx$ \textbf{0.019 seconds} ($\sim$52 images/second)

This demonstrates efficient inference performance on a single GPU.

\subsection{Sample Detection Results}

Sample detection results with visualized bounding boxes are saved in \texttt{pretrained/viz/}. The visualizations show that the model successfully detects objects from all 20 PASCAL VOC categories with appropriate confidence scores and localization.

Figure \ref{fig:sample} shows an example detection result where the model correctly identifies a sheep with high confidence. The purple bounding box indicates the predicted object location.

\begin{figure}[h]
\centering
\fbox{\textit{[Sample detection image: pretrained/viz/000321.png]}}
\caption{Example detection result showing a sheep detected by the FCOS model}
\label{fig:sample}
\end{figure}

\section{Key Implementation Challenges}

During implementation, we encountered and resolved several critical issues:

\subsection{Score Computation}

Initial implementation used direct multiplication of classification and centerness scores without the square root. This resulted in very low mAP (8.6\%). After verifying against official implementations, we corrected this to use $\sqrt{\text{cls} \times \text{ctr}}$, which significantly improved performance.

\subsection{Coordinate System}

The point generator creates coordinates using \texttt{meshgrid} with \texttt{indexing="ij"}, which produces grid\_x varying along rows (y-axis) and grid\_y varying along columns (x-axis). To maintain standard (x, y) coordinate convention, we stack the grids as \texttt{[grid\_y, grid\_x]} in the point generator. This ensures consistent coordinate interpretation throughout the inference pipeline.

\section{Conclusion}

We have successfully implemented the FCOS inference procedure, including:
\begin{itemize}
    \item Classification and regression heads with proper initialization
    \item Complete inference pipeline with score computation, box decoding, and NMS
    \item Verification against official implementations
\end{itemize}

The evaluation results confirm correct implementation with mAP@0.5 of 60.6\%, matching the expected performance of the pretrained model.

\end{document}
